{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.2: Inspect BeatAML Data Files in Detail\n",
    "\n",
    "**Project:** AML Multi-Omics Integration  \n",
    "**Date:** 2025-10-02  \n",
    "**Objective:** Comprehensive inspection of each downloaded BeatAML file\n",
    "\n",
    "This notebook will analyze:\n",
    "- File format and structure\n",
    "- Dimensions (rows √ó columns)\n",
    "- Column names and data types\n",
    "- First 10 rows preview\n",
    "- Basic statistics\n",
    "- Missing data analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Libraries imported successfully\n",
      "Pandas version: 2.3.2\n",
      "NumPy version: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "print(\"‚úì Libraries imported successfully\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: D:\\Projects\\Project_AML\n",
      "Data directory: D:\\Projects\\Project_AML\\01_Data\\BeatAML_Downloaded_Data\n",
      "Output directory: D:\\Projects\\Project_AML\\03_Results\\02_QC_Reports\n"
     ]
    }
   ],
   "source": [
    "# Set paths\n",
    "project_root = Path.cwd().parent.parent\n",
    "data_dir = project_root / \"01_Data\" / \"BeatAML_Downloaded_Data\"\n",
    "output_dir = project_root / \"03_Results\" / \"02_QC_Reports\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def format_bytes(size):\n",
    "    \"\"\"Convert bytes to human readable format.\"\"\"\n",
    "    for unit in ['B', 'KB', 'MB', 'GB']:\n",
    "        if size < 1024.0:\n",
    "            return f\"{size:.2f} {unit}\"\n",
    "        size /= 1024.0\n",
    "    return f\"{size:.2f} TB\"\n",
    "\n",
    "def get_basic_stats(df, col):\n",
    "    \"\"\"Get basic statistics for a column.\"\"\"\n",
    "    stats = {}\n",
    "    try:\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            stats['min'] = df[col].min()\n",
    "            stats['max'] = df[col].max()\n",
    "            stats['mean'] = df[col].mean()\n",
    "            stats['median'] = df[col].median()\n",
    "        else:\n",
    "            stats['unique'] = df[col].nunique()\n",
    "            top_val = df[col].value_counts().head(1)\n",
    "            if len(top_val) > 0:\n",
    "                stats['top_value'] = top_val.index[0]\n",
    "                stats['top_count'] = top_val.values[0]\n",
    "    except:\n",
    "        pass\n",
    "    return stats\n",
    "\n",
    "print(\"‚úì Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. File Inspection Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_file(filepath, filename):\n",
    "    \"\"\"Inspect a single data file and display results.\"\"\"\n",
    "    \n",
    "    display(Markdown(f\"\\n---\\n## üìä {filename}\\n---\"))\n",
    "    \n",
    "    # File size\n",
    "    file_size = os.path.getsize(filepath)\n",
    "    print(f\"üìÅ File Size: {format_bytes(file_size)}\")\n",
    "    \n",
    "    # Determine file type and read\n",
    "    try:\n",
    "        if filename.endswith('.xlsx'):\n",
    "            print(\"üìã File Format: Excel (.xlsx)\")\n",
    "            df = pd.read_excel(filepath)\n",
    "        elif filename.endswith('.txt'):\n",
    "            # Try to detect delimiter\n",
    "            with open(filepath, 'r') as f:\n",
    "                first_line = f.readline()\n",
    "                delimiter = '\\t' if '\\t' in first_line else ','\n",
    "            \n",
    "            format_name = \"Tab-delimited\" if delimiter == '\\t' else \"Comma-separated\"\n",
    "            print(f\"üìã File Format: {format_name} text\")\n",
    "            df = pd.read_csv(filepath, sep=delimiter, low_memory=False)\n",
    "        else:\n",
    "            print(\"‚ùå Unknown file format\")\n",
    "            return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR reading file: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Dimensions\n",
    "    n_rows, n_cols = df.shape\n",
    "    print(f\"üìê Dimensions: {n_rows:,} rows √ó {n_cols:,} columns\")\n",
    "    print()\n",
    "    \n",
    "    # Column information table\n",
    "    display(Markdown(\"### Column Information\"))\n",
    "    \n",
    "    col_info = []\n",
    "    for col in df.columns:\n",
    "        dtype = str(df[col].dtype)\n",
    "        n_missing = df[col].isna().sum()\n",
    "        pct_missing = (n_missing / len(df)) * 100\n",
    "        \n",
    "        stats = get_basic_stats(df, col)\n",
    "        \n",
    "        if 'min' in stats:\n",
    "            info = f\"Range: [{stats['min']:.2f}, {stats['max']:.2f}], Mean: {stats['mean']:.2f}\"\n",
    "        elif 'unique' in stats:\n",
    "            info = f\"{stats['unique']:,} unique values\"\n",
    "        else:\n",
    "            info = \"-\"\n",
    "        \n",
    "        col_info.append({\n",
    "            'Column': col[:40] + '...' if len(col) > 40 else col,\n",
    "            'Type': dtype,\n",
    "            'Missing': f\"{n_missing:,} ({pct_missing:.1f}%)\",\n",
    "            'Info': info\n",
    "        })\n",
    "    \n",
    "    col_df = pd.DataFrame(col_info)\n",
    "    display(col_df)\n",
    "    \n",
    "    # First 10 rows preview\n",
    "    display(Markdown(\"### First 10 Rows Preview\"))\n",
    "    display(df.head(10))\n",
    "    \n",
    "    # Overall missing data\n",
    "    total_cells = n_rows * n_cols\n",
    "    total_missing = df.isna().sum().sum()\n",
    "    pct_missing = (total_missing / total_cells) * 100\n",
    "    \n",
    "    print(f\"\\nüìä Overall Missing Data: {total_missing:,} / {total_cells:,} ({pct_missing:.2f}%)\")\n",
    "    \n",
    "    # Data quality issues\n",
    "    display(Markdown(\"### Data Quality Check\"))\n",
    "    \n",
    "    quality_issues = []\n",
    "    \n",
    "    # Check for duplicate rows\n",
    "    n_duplicates = df.duplicated().sum()\n",
    "    if n_duplicates > 0:\n",
    "        quality_issues.append(f\"‚ö†Ô∏è {n_duplicates:,} duplicate rows found\")\n",
    "    else:\n",
    "        quality_issues.append(\"‚úÖ No duplicate rows\")\n",
    "    \n",
    "    # Check for columns with all missing data\n",
    "    all_missing_cols = [col for col in df.columns if df[col].isna().all()]\n",
    "    if all_missing_cols:\n",
    "        quality_issues.append(f\"‚ö†Ô∏è {len(all_missing_cols)} columns with all missing data\")\n",
    "    else:\n",
    "        quality_issues.append(\"‚úÖ No columns with all missing data\")\n",
    "    \n",
    "    # Check for columns with >50% missing\n",
    "    high_missing_cols = [col for col in df.columns if (df[col].isna().sum() / len(df)) > 0.5]\n",
    "    if high_missing_cols:\n",
    "        quality_issues.append(f\"‚ö†Ô∏è {len(high_missing_cols)} columns with >50% missing data\")\n",
    "        for col in high_missing_cols[:5]:  # Show first 5\n",
    "            pct = (df[col].isna().sum() / len(df)) * 100\n",
    "            quality_issues.append(f\"  ‚Ä¢ {col} ({pct:.1f}% missing)\")\n",
    "        if len(high_missing_cols) > 5:\n",
    "            quality_issues.append(f\"  ‚Ä¢ ... and {len(high_missing_cols)-5} more\")\n",
    "    else:\n",
    "        quality_issues.append(\"‚úÖ No columns with >50% missing data\")\n",
    "    \n",
    "    for issue in quality_issues:\n",
    "        print(issue)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"‚úì Inspection function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspect All Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files to inspect\n",
    "files_to_inspect = [\n",
    "    'beataml_expression.txt',\n",
    "    'beataml_drug_auc.txt',\n",
    "    'beataml_clinical.xlsx',\n",
    "    'beataml_mutations.txt',\n",
    "    'beataml_raw_inhibitor.txt',\n",
    "    'beataml_drug_families.xlsx'\n",
    "]\n",
    "\n",
    "display(Markdown(f\"# BeatAML Data Files - Detailed Inspection\"))\n",
    "display(Markdown(f\"**Inspection Time:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"))\n",
    "display(Markdown(f\"**Total Files:** {len(files_to_inspect)}\"))\n",
    "\n",
    "# Store loaded dataframes\n",
    "data_dict = {}\n",
    "\n",
    "for filename in files_to_inspect:\n",
    "    filepath = data_dir / filename\n",
    "    if filepath.exists():\n",
    "        df = inspect_file(filepath, filename)\n",
    "        if df is not None:\n",
    "            # Store with simplified name\n",
    "            key = filename.replace('beataml_', '').replace('.txt', '').replace('.xlsx', '')\n",
    "            data_dict[key] = df\n",
    "    else:\n",
    "        display(Markdown(f\"\\n---\\n## ‚ùå {filename}\\n**Status:** File not found\\n---\"))\n",
    "\n",
    "print(f\"\\n‚úÖ Inspection complete! Loaded {len(data_dict)} datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary Statistics Across All Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"## üìà Summary Across All Files\"))\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for key, df in data_dict.items():\n",
    "    n_rows, n_cols = df.shape\n",
    "    total_cells = n_rows * n_cols\n",
    "    total_missing = df.isna().sum().sum()\n",
    "    pct_missing = (total_missing / total_cells) * 100 if total_cells > 0 else 0\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Dataset': key,\n",
    "        'Rows': f\"{n_rows:,}\",\n",
    "        'Columns': n_cols,\n",
    "        'Total Cells': f\"{total_cells:,}\",\n",
    "        'Missing (%)': f\"{pct_missing:.2f}%\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "display(summary_df)\n",
    "\n",
    "print(f\"\\n‚úÖ Total datasets analyzed: {len(data_dict)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Detailed text report is generated by the Python script\n",
    "# Here we save a summary CSV\n",
    "\n",
    "summary_csv = output_dir / \"data_files_summary.csv\"\n",
    "summary_df.to_csv(summary_csv, index=False)\n",
    "\n",
    "print(f\"‚úÖ Summary saved to: {summary_csv}\")\n",
    "print(f\"\\nüìù For detailed inspection report, run: python 02_inspect_data.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Access\n",
    "\n",
    "All datasets are now loaded and available in the `data_dict` dictionary:\n",
    "\n",
    "- `data_dict['expression']` - Gene expression data\n",
    "- `data_dict['drug_auc']` - Drug response AUC values\n",
    "- `data_dict['clinical']` - Clinical annotations\n",
    "- `data_dict['mutations']` - Mutation calls\n",
    "- `data_dict['raw_inhibitor']` - Raw drug response data\n",
    "- `data_dict['drug_families']` - Drug family information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Access a specific dataset\n",
    "print(\"Available datasets:\")\n",
    "for key in data_dict.keys():\n",
    "    print(f\"  - {key}: {data_dict[key].shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
